<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2019-02-01 Fri 12:22 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>RMS Titanic: Machine Learning from Disaster</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="Myrthe Boone" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2018 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">RMS Titanic: Machine Learning from Disaster</h1>

<div class="figure">
<p><img src="./titanicfrontpage.png" alt="titanicfrontpage.png" width="400px" />
</p>
</div>

<p>
\newpage
</p>
<div id="outline-container-org9f16881" class="outline-2">
<h2 id="org9f16881"><span class="section-number-2">1</span> Preface</h2>
<div class="outline-text-2" id="text-1">
<p>
In this paper we will have a look at the passengers on board of the Titanic. We will try to find the characteristics of people who were most likely to survive the disaster using machine learning techniques. Using these characteristics, we will make a prediction whether passengers have survived or not. The goal of this paper is not to make predictions about the future or about disasters in general. The results of our research can teach us something about the circumstances during the time that the Titanic sank. The passengers all played a different part in society back in those days. It teaches us something about the civilization at the time. 
</p>

<p>
Moreover, this paper is written because I wanted to learn something about machine learning and programming using Python. I want to study engineering at TU Eindhoven. It will come in handy if I already know a thing or two about programming in Python. Python is a programming language that is becoming more and more popular for things like data analysis and I am certain that I will use it more often in the future.  
</p>

<p>
I would like to give a special thanks to the following people. My father, who has helped me learn programming in Python and has taught me the basics of machine learning. Thank you for believing in me. Likewise, I would like to thank my supervisor mr. Kampwart for being enthusiastic and keeping me motivated. Lastly, I wanted to give thanks to DataCamp for providing me with courses on programming in Python and to Kaggle.com for the dataset of the Titanic. 
</p>

<p>
\newpage
</p>

<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org9f16881">1. Preface</a></li>
<li><a href="#org76c87ce">2. Introduction</a>
<ul>
<li><a href="#orgbf94f27">2.1. Machine Learning</a></li>
<li><a href="#org0806e5c">2.2. Different types of Machine Learning</a></li>
<li><a href="#org363a6d6">2.3. Algorithms</a></li>
<li><a href="#org129f902">2.4. Main questions and sub-questions</a></li>
</ul>
</li>
<li><a href="#org7317465">3. Preparation</a>
<ul>
<li><a href="#orgadab3d5">3.1. A first look at the dataset</a></li>
<li><a href="#org81b17bf">3.2. Preprocessing techniques</a></li>
</ul>
</li>
<li><a href="#orgd3ef9e3">4. Fitting models</a>
<ul>
<li><a href="#org7e41ec0">4.1. The first algorithm: KNearestNeighbors</a></li>
<li><a href="#org17606c2">4.2. The second algorithm: Logistic Regression</a></li>
</ul>
</li>
<li><a href="#orgc40f262">5. Conclusion</a></li>
<li><a href="#org19ef723">6. Discussion</a></li>
<li><a href="#org6973c2f">7. Postface</a></li>
<li><a href="#org9d8e3ce">8. References</a></li>
<li><a href="#org3ec1bbb">9. Appendix</a>
<ul>
<li><a href="#org128917f">9.1. Some more plots</a></li>
<li><a href="#orgbb5676d">9.2. Some more cross-validation</a></li>
</ul>
</li>
</ul>
</div>
</div>

<p>
\newpage
</p>
</div>
</div>


<div id="outline-container-org76c87ce" class="outline-2">
<h2 id="org76c87ce"><span class="section-number-2">2</span> Introduction</h2>
<div class="outline-text-2" id="text-2">
<p>
The RMS Titanic was the largest ship on water during that time and it was the second of three  ocean liners operated by the White Star Line.<sup><a id="fnr.1" class="footref" href="#fn.1">1</a></sup> The ship consisted of nine decks: the boat deck, seven decks labelled from A to G which carried the passengers and the Orlop Deck which was below the waterline. The liner had a height of 175 feet and a breadth of 92 feet.<sup><a id="fnr.2" class="footref" href="#fn.2">2</a></sup> 
</p>

<p>
In the year 1912 on the 15th of April one of the most infamous ships in history would crash into an iceberg and sink in the North Atlantic Ocean. During its maiden voyage from Southhampton to New York City on the 14th of April at 11:40 p.m. ship's time, the lookout sounded the alarm  when a massive clump of solid ice caught his attention. The first mate had seen the iceberg before the lookout did and tried to turn the ship around. Unfortunately, he was too late. Forty seconds later at a high speed the Titanic collided with a huge rock made of ice with a weight of 30 million kilograms. The collision caused a series of holes along the side of the hull.<sup><a id="fnr.3" class="footref" href="#fn.3">3</a></sup> Six of the watertight compartments were filled with water, whereas the ship could only sail on with a maximum of four compartments flooded. Consequently, the Titanic was doomed to sink. The crew understood they needed to act fast. They deployed the evacuation program. The ship carried twenty lifeboats. In principle the protocol "women and children first" was followed. However, this did not apply equally for everyone on board. The chance of being saved was  dependent on the class in which one travelled and the place where one found oneself during the evacuation. Around 2:20 a.m. parts of the Titanic broke off and sunk with one thousand people still on board. On deck were some of the richest people in the world, movie stars, school teachers and immigrants, who were hoping to find a new life in New York City. A life that some of them  would never find. Two hours after the ship sank, the liner RMS Carpathia arrived and saved an estimated 705 people.<sup><a id="fnr.4" class="footref" href="#fn.4">4</a></sup> The sinking of the RMS Titanic killed 1502 out of the 2224 people on board, crew members as well as passengers.<sup><a id="fnr.5" class="footref" href="#fn.5">5</a></sup>
</p>


<div id="org996b423" class="figure">
<p><img src="./TitanicProfile.png" alt="TitanicProfile.png" width="400px" /> 
</p>
<p><span class="figure-number">Figure 2: </span>Profile of RMS Titanic with the decks indicated</p>
</div>

<p>
The Titanic may be one of the most iconic ships in history, its story known the world over.<sup><a id="fnr.6" class="footref" href="#fn.6">6</a></sup> The tragedy has led to better safety regulations for ships and inspired numerous expeditions, movies, books, plays and characters.
</p>

<p>
Many passengers have lost their lives due to the fact that there were not enough lifeboats. Luck played a part in surviving this disaster. Moreover, some groups had an advantage compared to other groups. For instance, the "women and children first" policy left a relatively larger number of (older) men aboard. Similarly, speculations can be made regarding the advantage of the elderly aboard the Titanic. On the one hand it seems logical that the seniors were helped to the lifeboats because of a policy similar to the one about women and children. Older people are not as physically fit as the rest of the passengers, therefore they need to be assisted. On the other hand however, were the elderly the ones left behind as a result of their physical condition. They would have had more trouble climbing from the lowest deck to the boat deck. Finally, some people travelling first class might have had a better chance at surviving as well. The passengers were able to choose between three classes, varying in price and comfort. There was also a correlation between these three classes and wealth and social class. Most of the people travelling first class were, for example, businessmen, politicians and bankers. Second class travellers included professors, authors and tourists, members of the middle class. Emigrant workers moving to the United States and Canada travelled third class. In general, people travelling first class were closer to the boat deck and had, therefore, more chance to escape the flooding of the cabins (see Figure <a href="#tab:titanicdeckplanone">tab:titanicdeckplanone</a> and Figure <a href="#tab:titanicdeckplantwo">tab:titanicdeckplantwo</a>). They could get to the life boats faster than people whose cabins were on one of the lower decks. The price paid for a ticket is correlated with class. Tickets for travelling first class were in general more expensive than tickets for travelling second or third class. 
</p>


<div id="org7dd1d6c" class="figure">
<p><img src="./Deck2.png" alt="Deck2.png" width="300px" /> 
</p>
<p><span class="figure-number">Figure 3: </span>Deckplan of the Titanic</p>
</div>


<div id="org7061b7f" class="figure">
<p><img src="./Deck3.png" alt="Deck3.png" width="300px" /> 
</p>
<p><span class="figure-number">Figure 4: </span>Deckplan of the Titanic</p>
</div>


<p>
In this paper we will take a look at the characteristics of people who were more likely to survive the demise of the Titanic with the help of machine learning. We will predict the chances of survival of certain groups of passengers. In addition, we will see whether our expectation that children, women and rich people had an advantage indeed is correct. 
</p>
</div>

<div id="outline-container-orgbf94f27" class="outline-3">
<h3 id="orgbf94f27"><span class="section-number-3">2.1</span> Machine Learning</h3>
<div class="outline-text-3" id="text-2-1">
<p>
For the past 15 years, scientists have tried to make computers learn new things from given data with the help of machine learning. The definition of machine learning given by a professor at Stanford University is as follows: "Machine learning is the science of getting computers to act without being explicitly programmed."<sup><a id="fnr.7" class="footref" href="#fn.7">7</a></sup> It consists of giving computers the ability to learn and make decisions from data. These machine learning techniques are used to build predictive models. To illustrate, we will discuss some examples. 
</p>

<p>
First, consider spam emails that are sent to everyone who has an emailaccount. Whether the email is from a lottery telling you you have won a $1-million prize or from an unknown travel-agency offering you a trip to an exclusive resort for little money. It does not matter what the email looks like, your computer is able to distinguish the spam from your usual emails and places the spam in the spam folder of your account. The computer can detect the elements of spam, find patterns and compares the found patterns to new mail. Spam tends to have characteristic elements such as spelling mistakes, an originating address in Nigeria or claims that it needs your bank information. Second, huge tech giants such as Google, Netflix and Spotify use machine learning. The algorithms of these firms offer recommendations and suggestions based on previous user searches, movies watched and songs listened to, exactly because they can recognise a pattern in these cases.<sup><a id="fnr.8" class="footref" href="#fn.8">8</a></sup> Maybe one of the best known examples is AlphaGo. The computer programm developed by Google DeepMind in London to play the the boardgame Go.<sup><a id="fnr.9" class="footref" href="#fn.9">9</a></sup> In October 2015, AlphaGo became the first computer Go program to beat a human professional Go player. It was trained on moves of expert players from recorded historical games, a database of around 30 million moves. The algorithm used these moves to mimic human play by attempting to match these moves. Moreover, machine learning is making a breakthrough in the medical field as well. Artificial Intelligence (AI) pioneer Regina Barzilay carried out research and is now teaching machines to hunt down cancer. Experienced doctors have only a limited amount of patients' experience. Curing cancer is now more a trial-and-error process. With the help of machine learning people can be diagnosed faster and can be cured with the appropriate treatment.<sup><a id="fnr.10" class="footref" href="#fn.10">10</a></sup>   
</p>

<p>
A lot of different machine learning techniques exist. In this paper we will discuss two examples.
</p>
</div>
</div>


<div id="outline-container-org0806e5c" class="outline-3">
<h3 id="org0806e5c"><span class="section-number-3">2.2</span> Different types of Machine Learning</h3>
<div class="outline-text-3" id="text-2-2">
<p>
Machine learning can be divided in roughly three categories: reinforcement, unsupervised and supervised learning. The latter two will be discussed but we ignore here reinforcement learning. We ask ourselves the questions how does the computer know it is getting better or not, and how does it know how to improve? Different answers to these questions lead to different types of machine learning techniques; see Figure <a href="#tab:types">tab:types</a>. 
</p>


<div id="org743d69e" class="figure">
<p><img src="./typesmachinelearning.png" alt="typesmachinelearning.png" width="300px" />
</p>
<p><span class="figure-number">Figure 5: </span>An illustration of the different types of machine learning</p>
</div>


<p>
<b>Unsupervised learning</b>
This is a version of machine learning where the computer has to uncover patterns from unlabeled data. Correct labels are not provided. The algorithm has to identify similarities between the inputs. This way the inputs that have something in common are categorised together.<sup><a id="fnr.11" class="footref" href="#fn.11">11</a></sup>
</p>

<p>
For instance, grouping customers in categories based on buying behaviour without knowing in advance what these categories might be. 
</p>

<p>
<b>Supervised learning</b>
The majority of machine learning uses supervised learning and this is what we will be using as well. Whereas unsupervised learning has to make decisions from data that is not labeled (the correct responses are not provided), supervised machine learning deals with labeled data. The correct answers are already provided in the data. The algorithm generalises to respond correctly to all possible inputs, based on this training. The computer is provided with a specific input combined with the correct output or prediction. This way, the machine is trained to see the connections between the input and the right output. When a computer has had enough training or has been provided with enough data points, it will make less mistakes with every try.<sup><a id="fnr.12" class="footref" href="#fn.12">12</a></sup>
</p>

<p>
The Titanic task is an example of supervised learning. We know who has survived the disaster and who has not. This way we can train our computer on the dataset. Consequently, the computer learns to connect particular variables to the fact if someone has survived or not. Given a new person, of whom we don't know if he or she has survived it, the computer can make a prediction. We can produce the chances of survival for particular variables, e.g. gender, class etc. The goal is to predict the target variable, in this case 1 or 0 representing survived or not survived respectively in our Titanic dataset, given the predictor variables, such as class, gender, age, siblings etc. 
</p>

<p>
We can distinguish two different types of supervised learning: 
</p>
<ul class="org-ul">
<li><b>Classification</b>: the target variable consists of categories. Predicting survival on the Titanic is a classification problem. We have to classify, based on our predictor variables, if a person belongs to the class of survived (1) or not survived (0). This is called binary classification.</li>
<li><b>Regression</b>: the target variable is continuous. For instance, a dataset containing housing price data like the year the house was built, number of bedrooms, acreage. There is a price associated with each house. The goal is to predict the price of a house, given these variables. Since price is a continuous variable, this problem is an example of regression.</li>
</ul>
</div>
</div>


<div id="outline-container-org363a6d6" class="outline-3">
<h3 id="org363a6d6"><span class="section-number-3">2.3</span> Algorithms</h3>
<div class="outline-text-3" id="text-2-3">
<p>
To illustrate supervised machine learning, we use two different algorithms. Training our model on the data using an algorithm is called 'fitting' a model to the data. Fitting means minimizing the classification mistakes that we make. 
</p>

<p>
We split our data into a training and test set. We fit our model to the training data and predict on the test set. We do this in order to prevent the problem of overfitting. Overfitting means that our computer finds patterns in the data which are valid in our dataset but not representative for the population as a whole. So how does splitting our dataset solve this problem? We let our computer predict on the dataset it has never seen before, i.e. the test set. This way we can see whether our model fitting on the training set leads to overfitting on the test set. 
</p>

<p>
A second problem that can occur is underfitting. This means that the model misses patterns that are actually present in the data. We have to find a balance between this over- and underfitting. 
</p>
</div>


<div id="outline-container-orgcfbc10c" class="outline-4">
<h4 id="orgcfbc10c"><span class="section-number-4">2.3.1</span> KNearestNeighbors</h4>
<div class="outline-text-4" id="text-2-3-1">
<p>
To begin with, we use the so-called KNearestNeighbors algorithm. It predicts a label of a datapoint by looking at the 'k' closest labelled data points. KNN takes a majority vote on what label an undecided point has to have. For instance, when we want to decide if a dot on the map in Figure <a href="#tab:knn">tab:knn</a> is a blue square or a red triangle, we can choose our 'k' as 3. With choosing our 'k', we create a set of decision boundaries. Our computer will look at the three closest datapoints to classify our undecided point. If two or more of those three are blue squares, it classifies our undecided point as a blue square. If two or more of those three points are red triangles, it classifies our undecided point as a red triangle. The trick is to choose the right value for 'k'. Choosing too large a value for 'k', leads to underfitting. This creates a smoother decision boundary. To see this, imagine that 'k' equals \(n-1\), where \(n\) denotes the number of observations. Then, everything becomes one and the same colour. This way we have a less complex model, because our algorithm generalizes too much and uses too little information. On the other side, choosing too small a value for 'k' leads to overfitting. Consequently, our model is more complex and creates a more erratic boundary between different labels. We use 'too much' information and our model becomes less reliable.  Finding the right 'k' is a combination of using other algorithms to find 'k' and trial-and-error.<sup><a id="fnr.13" class="footref" href="#fn.13">13</a></sup>
</p>


<div id="org0c55953" class="figure">
<p><img src="./KnnClassification.png" alt="KnnClassification.png" width="110px" /> 
</p>
<p><span class="figure-number">Figure 6: </span>Illustration of the algorithm called KNearestNeighbors</p>
</div>
</div>
</div>

<div id="outline-container-orgd1ca238" class="outline-4">
<h4 id="orgd1ca238"><span class="section-number-4">2.3.2</span> Logistic regression</h4>
<div class="outline-text-4" id="text-2-3-2">
<p>
Second, we use an algorithm called logistic regression (logreg). The name can be misleading because logreg is commonly used for classification problems, not regression. It outputs probabilities. For example, if the dataset consists of \(n\) different classes, the algorithm calculates the chance that one specific case is classified as belonging to one of these \(n\) classes. With the Titanic data, we have \(n=2\). Therefore, we are dealing with a binary classification problem.<sup><a id="fnr.14" class="footref" href="#fn.14">14</a></sup> This implies the following: if we find \(p>0.5\), the variable is classified as 1, the passenger has survived the disaster; when we see \(p<0.5\), it is classified as 0, the passenger has not survived. 
</p>

<p>
To explain the principle of logistic regression, we will have a look at a linear function first:
</p>

\begin{equation}
y=ax+b
\end{equation} 

<p>
In this case there is only one predictor variable, \(a\) and \(b\) are the parameters of our model. We want to fit a line to the data. Fitting, in this case, consists of choosing a slope \(a\) and an intercept \(b\). Our Titanic dataset has more than one feature, because we have more than one predictor variable. Using linear regression, our line will look something like this, where each \(x_i\) represents a different predictor variable. 
</p>

\begin{equation}
y=a_1x_1+a_2x_2+ \dots + a_nx_n+b+\varepsilon  
\end{equation}

<p>
By calculating the vertical distance between each data point and the line, we can get an impression of how accurate our model is. This distance is called the residual (\(\varepsilon\)). One option is to minimze the sum of the residuals. However, this will not work because large positive values will cancel out large negative values. Consequently, shifting the line upwards will always reduce the sum of the residuals making the sum of the residuals \(-\infty\), which is the lowest value possible. So, to make sure that our line is as close to the actual data as possible, we calculate the sum of squared residuals (see Figure <a href="#tab:ols">tab:ols</a> and see Equation \ref{eq:residual}). This is called Ordinary Least Squares (OLS). When we call fit on our logistic regression model in scitkitlearn, it performs this OLS under the hood. Scikitlearn is a popular machine learning library for Python, which we will use to train our computer (see Footnote \ref{fn:datacamp}).
</p>

\begin{equation}
\label{eq:residual}
\sum_{i=1}^{n}\varepsilon^2_{i}
\end{equation}




<div id="org82c5ac8" class="figure">
<p><img src="./Residual.png" alt="Residual.png" width="300px" />
</p>
<p><span class="figure-number">Figure 7: </span>Ordinary Least Squares: Minimize sum of squares of residuals</p>
</div>

<p>
The red lines in Figure <a href="#tab:ols">tab:ols</a> represent \(\varepsilon_{i}\). We use logistic regression, because our target variable is not continuous: our variable is either 0 or 1. The logistic function \(\sigma(y)\) is defined as follows:
</p>

\begin{equation}
\label{eq:2}
\sigma(y) = \frac{e^y}{1+e^y}
\end{equation}

<p>
Because we have three variables(i.e. age, gender and class), \(y\) in this case is of the form:
</p>

\begin{equation}
y=a_1x_1+a_2x_2+a_3x_3+b+\varepsilon_{i} 
\end{equation}

<p>
The function in equation <a href="#eq:2">eq:2</a> is a sigmoid function (see Figure <a href="#tab:log">tab:log</a>), which takes any real input \(y\) (\(y\in{\rm I\!R}\)), and outputs a value between zero and one; a probability.
</p>


<div id="org71822b8" class="figure">
<p><img src="./LogisticCurve.png" alt="LogisticCurve.png" width="300px" />
</p>
<p><span class="figure-number">Figure 8: </span>The logistic function</p>
</div>

<p>
The underfitting and overfitting problem also applies to logistic regression. Adding more independent variables to our model increases the explained variance. Our model becomes more complex, as mentioned earlier. Using too few independent variables results in underfitting, where our model is too 'simple'. 
</p>

<p>
After using these two algorithms, we measure model performance. To do this, we use metrics such as accuracy. Accuracy is the fraction of correct predictions, think of the fraction of cases where the model correctly predicts that someone survived. How these metrics work, will be explained below. 
</p>

<p>
To sum up, we follow this procedure: We split our dataset into a training set and test set. Then we fit or train the classifier to the training set. Subsequently, we predict on the test set. In the end, we compare our predictions to the known labels and compute a metric of accuracy. 
</p>

<p>
\newpage
</p>
</div>
</div>
</div>

<div id="outline-container-org129f902" class="outline-3">
<h3 id="org129f902"><span class="section-number-3">2.4</span> Main questions and sub-questions</h3>
<div class="outline-text-3" id="text-2-4">
<p>
This research is motivated by the following main question and sub-questions: 
</p>

<p>
<b>Main question</b>
</p>

<p>
<i>Is it possible to make an accurate prediction whether the passengers on the Titanic survived the disaster or not using information about gender, class, age and fare?</i>
</p>

<p>
<b>Sub-questions</b> 
</p>

<ul class="org-ul">
<li><i>What is the influence of gender on the chance of surviving after the Titanic had sunk?</i></li>
<li><i>What is the influence of fare on the chance of surviving?</i></li>
<li><i>What is the influence of class on the chance of surviving?</i></li>
<li><i>What is the influence of age on the chance of surviving?</i></li>
</ul>

<p>
These questions lead to the following hypotheses:
</p>

<ul class="org-ul">
<li><b>Main question</b> : Yes this is possible, with the help of machine learning using the algorithms KNearestNeighbours and logistic regression.</li>
<li><b>Sub-questions</b> :

<ul class="org-ul">
<li>The survival rate of women is higher than the survival rate of men.</li>
<li>The survival rate of passengers who paid a higher fare is higher than those who paid less.</li>
<li>The survival rate of passengers who were travelling in a lower class (in this case first class is seen as lowest) is higher.</li>
<li>The survival rate of children and elderly is higher than the survival rate of the adults.</li>
</ul></li>
</ul>


<p>
\newpage
</p>
</div>
</div>
</div>
<div id="outline-container-org7317465" class="outline-2">
<h2 id="org7317465"><span class="section-number-2">3</span> Preparation</h2>
<div class="outline-text-2" id="text-3">
</div>
<div id="outline-container-orgadab3d5" class="outline-3">
<h3 id="orgadab3d5"><span class="section-number-3">3.1</span> A first look at the dataset</h3>
<div class="outline-text-3" id="text-3-1">
<p>
This adventure begins with importing a number of packages. We will use other packages as well, which we import along the way.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">import</span> numpy <span style="color: #0000FF;">as</span> np
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">import</span> pandas <span style="color: #0000FF;">as</span> pd
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">import</span> matplotlib.pyplot <span style="color: #0000FF;">as</span> plt
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">import</span> seaborn <span style="color: #0000FF;">as</span> sns
</pre>
</div>

<p>
The dataset is downloaded from <a href="https://www.kaggle.com/c/titanic/data">Kaggle</a><sup><a id="fnr.15" class="footref" href="#fn.15">15</a></sup> as <code>csv_file</code>. Thereafter, the data is read into a dataframe by using pandas <code>pd.read_csv</code>. 
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">data</span> = pd.read_csv(<span style="color: #008000;">'titanic.csv'</span>)
</pre>
</div>

<p>
Before we get started with our algorithms, we will have a look at our dataset. We perform some numerical Exploratory Data Analysis (EDA). This helps us analyse our dataset by giving a first impression of the data. 
</p>

<p>
Using the <code>.head()</code> method, we can see the first five rows of our dataset in Table <a href="#tab:table1">tab:table1</a>. A couple of questions come to
mind. Which variables play a role determining the probability of surviving the Titanic? As mentioned we are interested in gender but <code>Sex</code> is not a numeric value. How do we convert this in a way that our computer can deal with this variable? 
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   data.head()
</pre>
</div>

\begin{table}
\small
\begin{center}
\caption{\label{tab:table1}Head of the dataframe}
\begin{adjustwidth}{-2cm}{}
\begin{tabular}{|l|c|c|c|p{3cm}|l|c|p{1cm}|p{1cm}|p{1cm}|p{1cm}|l|p{0.5cm}|}
\toprule
\hline
{} &  Pass &  Surv &  Class &                                               Name &     Sex &   Age &  SibSp &  Parch &            Ticket &     Fare & Cabin & Emb \\
\midrule
\hline
 0 &            1 &         0 &       3 &                            Braund, Mr. Owen Harris &    male &  22.0 &      1 &      0 &         A/5 21171 &   7.2500 &   NaN &        S \\
 1 &            2 &         1 &       1 &  Cumings, Mrs. John Bradley (Florence Briggs Th... &  female &  38.0 &      1 &      0 &          PC 17599 &  71.2833 &   C85 &        C \\
 2 &            3 &         1 &       3 &                             Heikkinen, Miss. Laina &  female &  26.0 &      0 &      0 &  STON/ O2. 3101282 &   7.9250 &   NaN &        S \\
 3 &            4 &         1 &       1 &       Futrelle, Mrs. Jacques Heath (Lily May Peel) &  female &  35.0 &      1 &      0 &            113803 &  53.1000 &  C123 &        S \\
 4 &            5 &         0 &       3 &                           Allen, Mr. William Henry &    male &  35.0 &      0 &      0 &            373450 &   8.0500 &   NaN &        S \\
\bottomrule
\hline
\end{tabular}
\end{adjustwidth}
\end{center}
\end{table}


<p>
We have thirteen columns. <code>Pass</code> gives us the PassengerId. <code>Surv</code> shows us a 0 or 1, which stands for not survived and survived respectively. <code>SibSp</code> represents the number of siblings and <code>Parch</code> represents the number of parents of the passenger on board. <code>Emb</code> tells us the port of embarkation: <code>C</code> stands for Cherbourg, <code>Q</code> for Queenstown and <code>S</code> for Southampton. With the <code>.describe()</code> method we get the summary statistics of the numeric variables. The mean, standarddeviation etcetera are given in Table <a href="#tab:table2">tab:table2</a>.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   data.describe()
</pre>
</div>

\begin{table}
\small
\begin{center}
\caption{\label{tab:table2}Description of the dataframe}
\begin{tabular}{|l|c|c|c|c|c|c|c|}
\toprule
\hline
{} &  PassengerId &    Survived &      Pclass &         Age &       SibSp &       Parch &        Fare \\
\midrule
\hline
count &   891.000000 &  891.000000 &  891.000000 &  714.000000 &  891.000000 &  891.000000 &  891.000000 \\
mean  &   446.000000 &    0.383838 &    2.308642 &   29.699118 &    0.523008 &    0.381594 &   32.204208 \\
std   &   257.353842 &    0.486592 &    0.836071 &   14.526497 &    1.102743 &    0.806057 &   49.693429 \\
min   &     1.000000 &    0.000000 &    1.000000 &    0.420000 &    0.000000 &    0.000000 &    0.000000 \\
25\%   &   223.500000 &    0.000000 &    2.000000 &   20.125000 &    0.000000 &    0.000000 &    7.910400 \\
50\%   &   446.000000 &    0.000000 &    3.000000 &   28.000000 &    0.000000 &    0.000000 &   14.454200 \\
75\%   &   668.500000 &    1.000000 &    3.000000 &   38.000000 &    1.000000 &    0.000000 &   31.000000 \\
max   &   891.000000 &    1.000000 &    3.000000 &   80.000000 &    8.000000 &    6.000000 &  512.329200 \\
\bottomrule
\hline
\end{tabular}
\end{center}
\end{table}

<p>
In Table <a href="#tab:table2">tab:table2</a>, a couple of values stand out. The mean of <code>Survived</code> for example is 0.38. This indicates that 38% of the passengers on board has survived. Furthermore, the average age of people on board was around thirty years. The median age is twenty-eight and the eldest aboard was eighty years old. The maximum number of siblings on board of a passenger was 8 (family holiday!). We see that the maximum numbers of parents (<code>Parch</code>) on board is 6, which seems a bit odd&#x2026; Since we do not use this variable anyway we will not worry about it. Finally, the average <code>Fare</code> was 32 pounds. 
</p>

<p>
It is also possible to search for particular passengers in the dataset. Such as passengers with a particular name or with a particular age of say eighty years. 
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   data[data.Name == <span style="color: #008000;">'Braund, Mr. Owen Harris'</span>]
</pre>
</div>

<pre class="example">
   PassengerId  Survived  Pclass                     Name   Sex   Age  SibSp  \
0            1         0       3  Braund, Mr. Owen Harris  male  22.0      1   

   Parch     Ticket  Fare Cabin Embarked  
0      0  A/5 21171  7.25   NaN        S  

</pre>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PassengerId</th>
      <th>Survived</th>
      <th>Pclass</th>
      <th>Name</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Ticket</th>
      <th>Fare</th>
      <th>Cabin</th>
      <th>Embarked</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0</td>
      <td>3</td>
      <td>Braund, Mr. Owen Harris</td>
      <td>male</td>
      <td>22.0</td>
      <td>1</td>
      <td>0</td>
      <td>A/5 21171</td>
      <td>7.25</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
  </tbody>
</table>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span style="color: #9B9B9B; background-color: #EDEDED;"> </span> data[data.Age == 80]
</pre>
</div>

<pre class="example">
     PassengerId  Survived  Pclass                                  Name  \
630          631         1       1  Barkworth, Mr. Algernon Henry Wilson   

      Sex   Age  SibSp  Parch Ticket  Fare Cabin Embarked  
630  male  80.0      0      0  27042  30.0   A23        S  

</pre>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PassengerId</th>
      <th>Survived</th>
      <th>Pclass</th>
      <th>Name</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Ticket</th>
      <th>Fare</th>
      <th>Cabin</th>
      <th>Embarked</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>630</th>
      <td>631</td>
      <td>1</td>
      <td>1</td>
      <td>Barkworth, Mr. Algernon Henry Wilson</td>
      <td>male</td>
      <td>80.0</td>
      <td>0</td>
      <td>0</td>
      <td>27042</td>
      <td>30.0</td>
      <td>A23</td>
      <td>S</td>
    </tr>
  </tbody>
</table>
</div>

<p>
It is also possible to see who has paid more than 400 dollars for his or her ticket. We see that it is easy to make a selection in our dataset using the <code>&gt;</code> sign.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   data[data.Fare &gt; 400]
</pre>
</div>

<pre class="example">
     PassengerId  Survived  Pclass                                Name  \
258          259         1       1                    Ward, Miss. Anna   
679          680         1       1  Cardeza, Mr. Thomas Drake Martinez   
737          738         1       1              Lesurer, Mr. Gustave J   

        Sex   Age  SibSp  Parch    Ticket      Fare        Cabin Embarked  
258  female  35.0      0      0  PC 17755  512.3292          NaN        C  
679    male  36.0      0      1  PC 17755  512.3292  B51 B53 B55        C  
737    male  35.0      0      0  PC 17755  512.3292         B101        C  

</pre>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PassengerId</th>
      <th>Survived</th>
      <th>Pclass</th>
      <th>Name</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Ticket</th>
      <th>Fare</th>
      <th>Cabin</th>
      <th>Embarked</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>258</th>
      <td>259</td>
      <td>1</td>
      <td>1</td>
      <td>Ward, Miss. Anna</td>
      <td>female</td>
      <td>35.0</td>
      <td>0</td>
      <td>0</td>
      <td>PC 17755</td>
      <td>512.3292</td>
      <td>NaN</td>
      <td>C</td>
    </tr>
    <tr>
      <th>679</th>
      <td>680</td>
      <td>1</td>
      <td>1</td>
      <td>Cardeza, Mr. Thomas Drake Martinez</td>
      <td>male</td>
      <td>36.0</td>
      <td>0</td>
      <td>1</td>
      <td>PC 17755</td>
      <td>512.3292</td>
      <td>B51 B53 B55</td>
      <td>C</td>
    </tr>
    <tr>
      <th>737</th>
      <td>738</td>
      <td>1</td>
      <td>1</td>
      <td>Lesurer, Mr. Gustave J</td>
      <td>male</td>
      <td>35.0</td>
      <td>0</td>
      <td>0</td>
      <td>PC 17755</td>
      <td>512.3292</td>
      <td>B101</td>
      <td>C</td>
    </tr>
  </tbody>
</table>
</div>


<p>
Next we perform some visual EDA. We do this in order to have a look at possible correlations between variables and at how our data is distributed. We can make a couple of plots using the <code>matplotlib.pyplot</code> and <code>seaborn</code> packages. We need to keep in mind that these plots show correlations, not causality.  
</p>

<p>
Let's start with plotting <code>Age</code> against <code>Survived</code>. The result is Figure <a href="#tab:agesurvived">tab:agesurvived</a>. <code>Survived</code> is not a continuous variable, so we see two strokes of dots. Looking at the plot, we can conclude that there was someone of eighty who has survived. 
</p>
<div class="org-src-container">
<pre class="src src-ipython"><span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   plt.scatter(data.Age,data.Survived)
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   plt.xlabel(<span style="color: #008000;">'Age'</span>)
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   plt.ylabel(<span style="color: #008000;">'Survived'</span>)
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   plt.savefig (<span style="color: #008000;">'AgeSurvived.png'</span>)
</pre>
</div>

<p width="400px">
<img src="./AgeSurvived.png" alt="AgeSurvived.png" width="400px" />
Now we have a look at the relationship between class and price paid for a ticket in Figure <a href="#tab:classfare">tab:classfare</a>. It is likely that we will see some correlation. The line relating to first class has higher values than the ones relating to second and third class. Below we write the same code for plotting the scatter plots. We will not, however, show the code everytime because this would make it less readable. Hence first class tickets tend to be more expensive than second and third class tickets. 
</p>


<div id="orgdcc7add" class="figure">
<p><img src="./PclassFare.png" alt="PclassFare.png" width="400px" />
</p>
<p><span class="figure-number">Figure 9: </span>Plot of Class against Fare</p>
</div>

<p>
A couple of values stand out. We see that a passenger or more passengers travelling first class have paid more than 500 pounds for their ticket.
</p>

<p>
After we have plotted  <code>Fare</code> against <code>Survived</code>, we take a look at Figure <a href="#tab:faresurvived">tab:faresurvived</a>. 
</p>

<div id="org0c40b59" class="figure">
<p><img src="./FareSurvived.png" alt="FareSurvived.png" width="400px" />
</p>
<p><span class="figure-number">Figure 10: </span>Plot of Fare against Survived</p>
</div>

<p>
Between <code>Fare</code> and <code>Age</code> we can conclude that passengers younger than ten years have not paid a lot for their ticket as opposed to other passengers (see Figure <a href="#tab:fareage">tab:fareage</a>). People who paid more for their tickets were older. But not everyone who was older, has paid more for their tickets. 
</p>

<div id="org4133088" class="figure">
<p><img src="./FareAge.png" alt="FareAge.png" width="400px" />
</p>
<p><span class="figure-number">Figure 11: </span>Plot of Fare against Age</p>
</div>


<p>
If we plot a scatter matrix with the variables <code>Class</code>, <code>Fare</code> and <code>Age</code>, we get Figure <a href="#tab:scattermatrix">tab:scattermatrix</a>.   
\newpage
</p>
<div class="org-src-container">
<pre class="src src-ipython"><span style="color: #0000FF;">from</span> pandas.plotting <span style="color: #0000FF;">import</span> scatter_matrix

<span style="color: #BA36A5;">axs</span> = scatter_matrix(P_titanic[[<span style="color: #008000;">'Pclass'</span>,<span style="color: #008000;">'Fare'</span>,<span style="color: #008000;">'Age'</span>]], alpha=0.2, figsize=(10, 10), diagonal=<span style="color: #008000;">'hist'</span>)
plt.savefig(<span style="color: #008000;">'scatter.png'</span>)
</pre>
</div>


<div id="orgd25b3b7" class="figure">
<p><img src="./scatter.png" alt="scatter.png" width="400px" />
</p>
<p><span class="figure-number">Figure 12: </span>Scatter matrix with histograms on the diagonal</p>
</div>


<p>
<b>Diagonal scatter matrix</b> 
</p>

<p>
The scatter matrix plots all the combinations of our variables in the scatter plots. This gives us an overview. On the diagonal we see a histogram that represents the relative distribution of the variables. Looking at the histogram for <code>Age</code> for example, it shows how many people of each particular age group were on the Titanic. 
</p>


<p>
We plot a <code>binary Seaborn Counplot</code>. Plotting <code>Class</code> against <code>Survived</code>, we can see that there were more people in the third than in the first class. This makes it difficult to compare them to each other. One option is to calculate percentages. In general, we cannot draw a conclusion regarding survival probabilities. In the third class, more passengers died than survived. In the first class, more people survived than perished. The plot only shows us one variable. This is another reason why we cannot directly see the influence of class on the chance of survival. The effect of first class on the chance of survival can be different for a woman than for a man for example and men and women may not be distrubuted equally over the classes. This is because the variables have an influence on eachother as well. We will have a further look at this problem in the <a href="#orga288730">Discussion</a>.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   sns.<span style="color: #006FE0;">set</span>(style=<span style="color: #008000;">"darkgrid"</span>)
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">ax</span> = sns.countplot(x=<span style="color: #008000;">"Pclass"</span>,hue=<span style="color: #008000;">"Survived"</span>, data=data, palette=<span style="color: #008000;">"Set3"</span>)
</pre>
</div>


<div class="figure">
<p><img src="obipy-resources/b207b8ecf66cdede2d5455fb7467ce47-17581z4s.png" alt="b207b8ecf66cdede2d5455fb7467ce47-17581z4s.png" />
</p>
</div>

<p>
Here we see a plot with <code>Age</code> against <code>Survived</code>. We can see some blue points for the passengers of a younger age. Furthermore, a lot of people of middle age have not survived. This is caused to some extent by the fact that there were more passengers of middle age on board.
</p>





<div class="figure">
<p><img src="obipy-resources/b207b8ecf66cdede2d5455fb7467ce47-17581ADz.png" alt="b207b8ecf66cdede2d5455fb7467ce47-17581ADz.png" />
</p>
</div>

<p>
Here we see a plot of our <code>male_dummy</code>. <code>False</code> represents in this case the women on board of the Titanic. We see that there were more women who have survived than women who did not. <code>True</code> stands in this case for the men on board. We see that more men have perished than survived. One interpretation is that the "women and children first" policy was followed.
</p>


<div class="figure">
<p><img src="obipy-resources/b207b8ecf66cdede2d5455fb7467ce47-17581MoC.png" alt="b207b8ecf66cdede2d5455fb7467ce47-17581MoC.png" />
</p>
</div>


<p>
In the <a href="#orgb557d4b">Appendix</a> we provide more plots.  
</p>
</div>
</div>



<div id="outline-container-org81b17bf" class="outline-3">
<h3 id="org81b17bf"><span class="section-number-3">3.2</span> Preprocessing techniques</h3>
<div class="outline-text-3" id="text-3-2">
<p>
Now we have explored our dataset and have seen what it looks like, we make the following adjustments. These are called "preprocessing techniques". The package <code>scikitlearn</code> cannot work with non-numerical values like the values of <code>Sex</code>. We turn this into a male dummy. Moreover, for some observations variable values are missing. To ease the exposition we drop these observations. 
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">df_cleaned</span> = data.dropna()
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">df_cleaned</span>[<span style="color: #008000;">'male_dummy'</span>] = (df_cleaned.Sex == <span style="color: #008000;">'male'</span>) 
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">X</span> = df_cleaned[[<span style="color: #008000;">'Age'</span>,<span style="color: #008000;">'male_dummy'</span>, <span style="color: #008000;">'Pclass'</span>, <span style="color: #008000;">'SibSp'</span>, <span style="color: #008000;">'Fare'</span>]]
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">y</span> = df_cleaned[[<span style="color: #008000;">'Survived'</span>]]
</pre>
</div>

<p>
We "clean" our dataset for the first time to make it more suitable for
the packages we will be using. All rows with missing values, these are called
NaNs (this is short for Not a Number), are deleted. We delete these by using
<code>.dropna()</code>. There are other ways than deleting rows to handle this problem.
Such as, replacing the NaNs with the mean or interpolating. However, the
choice was made this time to delete these rows. Furthermore, we see that the
problem of the <code>Sex</code> column not being a numeric value is handled. The values
in the <code>Sex</code> column are changed into a boolean. A boolean is a datatype with
only two possible values, i.e. <code>True</code> or <code>False</code>. Males are given a <code>True</code> (1) and
the females are given a <code>False</code> (0). Next we have added a couple of variables
to <code>X</code>: <code>Age</code>, <code>male_dummy</code>, <code>Pclass</code>, <code>SibSp</code> and <code>Fare</code>. These are all numeric
values and therefore easy to use.
Here we see the cleaned dataframe in Table <a href="#tab:tabledfcleaned">tab:tabledfcleaned</a> with the new added column <code>male_dummy</code>. 
</p>
\begin{table}
\small
\begin{center}
\caption{\label{tab:tabledfcleaned}Head of the cleaned dataframe}
\begin{adjustwidth}{-2cm}{}
\begin{tabular}{|l|c|c|c|p{3cm}|l|c|p{1cm}|p{1cm}|p{1cm}|p{1cm}|l|l|p{1cm}|}
\toprule
\hline
{} &  Pass &  Surv &  Class &                                               Name &     Sex &   Age &  SibSp &  Parch &    Ticket &     Fare & Cabin & Emb &  male\_dummy \\
\midrule
\hline
1  &            2 &         1 &       1 &  Cumings, Mrs. John Bradley (Florence Briggs Th... &  female &  38.0 &      1 &      0 &  PC 17599 &  71.2833 &   C85 &        C &       False \\
3  &            4 &         1 &       1 &       Futrelle, Mrs. Jacques Heath (Lily May Peel) &  female &  35.0 &      1 &      0 &    113803 &  53.1000 &  C123 &        S &       False \\
6  &            7 &         0 &       1 &                            McCarthy, Mr. Timothy J &    male &  54.0 &      0 &      0 &     17463 &  51.8625 &   E46 &        S &        True \\
10 &           11 &         1 &       3 &                    Sandstrom, Miss. Marguerite Rut &  female &   4.0 &      1 &      1 &   PP 9549 &  16.7000 &    G6 &        S &       False \\
11 &           12 &         1 &       1 &                           Bonnell, Miss. Elizabeth &  female &  58.0 &      0 &      0 &    113783 &  26.5500 &  C103 &        S &       False \\
\bottomrule
\hline
\end{tabular}
\end{adjustwidth}
\end{center}
\end{table}



<p>
In this paper we will only have a look at the variables <code>Age</code>, <code>Sex</code>, <code>Class</code> and <code>Fare</code>, because we are interested in the effects of age, gende and wealth in the society of 1912. To simplify the dataset, we delete the columns with data we will not use. This new dataset is called <code>P_titanic</code>. The first five rows of this new dataset are given in Table <a href="#tab:ptitanichead">tab:ptitanichead</a>.
</p>
<div class="org-src-container">
<pre class="src src-ipython"><span style="color: #BA36A5;">P_titanic</span> = df_cleaned[[<span style="color: #008000;">'Pclass'</span>, <span style="color: #008000;">'Fare'</span>, <span style="color: #008000;">'Age'</span>, <span style="color: #008000;">'male_dummy'</span>]]
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">P_titanic.head()
</pre>
</div>


\begin{table}
\small
\begin{center}
\caption{\label{tab:ptitanichead}Head of P_titanic}
\begin{tabular}{|l|c|c|c|l|}
\toprule
\hline
{} &  Pclass &     Fare &   Age &  male\_dummy \\
\midrule
\hline
1  &       1 &  71.2833 &  38.0 &       False \\
3  &       1 &  53.1000 &  35.0 &       False \\
6  &       1 &  51.8625 &  54.0 &        True \\
10 &       3 &  16.7000 &   4.0 &       False \\
11 &       1 &  26.5500 &  58.0 &       False \\
\hline
\bottomrule
\end{tabular}
\end{center}
\end{table}



<p>
The corresponding column with the information about who has survived and who has not survived is called <code>q_titanic</code> and is given in Table <a href="#tab:qtitanichead">tab:qtitanichead</a>. 
</p>


<div class="org-src-container">
<pre class="src src-ipython"><span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">q_titanic</span> = df_cleaned.Survived
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">q_titanic.head()
</pre>
</div>

\begin{table}
\small
\begin{center}
\caption{\label{tab:qtitanichead}Head of q_titanic}
\begin{tabular}{|l|c|}
\toprule
\hline
{} &  Survived \\
\midrule
\hline
1  &         1 \\
3  &         1 \\
6  &         0 \\
10 &         1 \\
11 &         1 \\
\bottomrule
\hline
\end{tabular}
\end{center}
\end{table}

<p>
We see that numbers 2, 4, 5 etcetera are missing. This makes sense because we have deleted rows with missing values.
</p>

<p>
Using this data it is possible to make a graphic illustration of a prediction. We select the data concerning three of our variables, including the variable <code>Survived</code>, which we want to predict. 
</p>



<div id="org37ea54b" class="figure">
<p><img src="./survfare.png" alt="survfare.png" />
</p>
<p><span class="figure-number">Figure 16: </span>Survived of Fare vs. Age</p>
</div>



<p>
Here we see a graphic illustration of the relation between <code>Fare</code>, <code>Age</code>
and <code>Survived</code>. The relation is not very clear but we see that the higher the fare the more people survived and the higher the age the less people survived. However, we are also interested in the effect of gender and class. It is not possible to draw a reliable conclusion from this plot.
</p>

<p>
It is inconvenient to plot discrete variables such as <code>Class</code> and <code>Age</code>. It is harder to distinguish how many blue and green triangles there are in the plot. In the <a href="#orgb557d4b">Appendix</a> we provide more plots. 
</p>



<div id="org90d5834" class="figure">
<p><img src="./survclass.png" alt="survclass.png" />
</p>
<p><span class="figure-number">Figure 17: </span>Survived of Class vs. Age</p>
</div>



<p>
\newpage
</p>
</div>
</div>
</div>

<div id="outline-container-orgd3ef9e3" class="outline-2">
<h2 id="orgd3ef9e3"><span class="section-number-2">4</span> Fitting models</h2>
<div class="outline-text-2" id="text-4">
</div>
<div id="outline-container-org7e41ec0" class="outline-3">
<h3 id="org7e41ec0"><span class="section-number-3">4.1</span> The first algorithm: KNearestNeighbors</h3>
<div class="outline-text-3" id="text-4-1">
<p>
One way to approach our problem is using the algorithm called KNearestNeighbors (KNN). We import the classifier from the library <code>sklearn.neighbours</code>. 
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">from</span> sklearn.neighbors <span style="color: #0000FF;">import</span> KNeighborsClassifier
</pre>
</div>

<p>
We choose 6 neighbors. In KNN finding the value of \(k\) is not easy. A small value of k means that noise will have a higher influence on the result and a large value makes it computationally expensive. We will not spend a lot of time on finding the right \(k\) for the reason that the emphasis of this paper is on getting a general idea of how the algorithms work. 
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">knn</span> = KNeighborsClassifier(n_neighbors=6)
</pre>
</div>

<p>
We split our data into a training set and a test set. The
arguments allow us to specify the size of our training and test set. This and
the parameters will be varied to see which values gives the best
prediction. We find that our model performance is dependent on the way our data is split. If we choose our test size to be 0.2 and we compute our accuracy score, we get the following:   
</p>


<div class="org-src-container">
<pre class="src src-ipython"><span style="color: #0000FF;">from</span> sklearn.model_selection <span style="color: #0000FF;">import</span> train_test_split
<span style="color: #BA36A5;">P_titanic_train</span>, <span style="color: #BA36A5;">P_titanic_test</span>, <span style="color: #BA36A5;">q_titanic_train</span>, <span style="color: #BA36A5;">q_titanic_test</span> = \
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   train_test_split(P_titanic,q_titanic, test_size=0.2, random_state=42)
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>
</pre>
</div>

<p>
We fit our classifier on the training set and consequently predict on the test set.  
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span style="color: #9B9B9B; background-color: #EDEDED;"> </span>knn.fit(P_titanic_train, q_titanic_train)
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span><span style="color: #BA36A5;">prediction</span>= knn.predict(P_titanic_test) 

</pre>
</div>

<p>
If we compute our accuracy score, which is the fraction of correct predictions, we find the following value:
</p>

<div class="org-src-container">
<pre class="src src-ipython">knn.score(P_titanic_test, q_titanic_test)
</pre>
</div>

<pre class="example">
0.7027027027027027

</pre>

<p>
Which means that about 70% of our prediction are accurate. 
</p>

<p>
If we <code>print</code> our prediction, this is what it looks like: 
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span style="color: #0000FF;">print</span>(<span style="color: #008000;">'Prediction{}'</span>.<span style="color: #006FE0;">format</span>(prediction))
</pre>
</div>

<pre class="example">
Prediction[1 1 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1]


</pre>

<p>
This is a prediction for the first 38 passengers with his or her specific characteristics. If we take a look at the head of our <code>P_titanic_test</code> (Table <a href="#tab:tableptest">tab:tableptest</a>), we can see for whom the algorithm has predicted that he or she has survived. The third '1' corresponds with the passenger number 742 on the list. 
</p>

\begin{table}
\small
\begin{center}
\caption{\label{tab:tableptest}Head of the test set}
\begin{tabular}{|l|c|c|c|l|}
\toprule
\hline
{} &  Pclass &      Fare &   Age &  male\_dummy \\
\midrule
\hline
118 &       1 &  247.5208 &  24.0 &        True \\
251 &       3 &   10.4625 &  29.0 &       False \\
742 &       1 &  262.3750 &  21.0 &       False \\
544 &       1 &  106.4250 &  50.0 &        True \\
712 &       1 &   52.0000 &  48.0 &        True \\
\bottomrule
\hline
\end{tabular}
\end{center}
\end{table}

<p>
So number 742 has, according to our model, survived the disaster. The <code>PassengerID</code> of this passenger is 743, because the ID is one higher than the row number. 
</p>

<div class="org-src-container">
<pre class="src src-ipython">df_cleaned[df_cleaned.PassengerId == 743]
</pre>
</div>

<pre class="example">
     PassengerId  Survived  Pclass                                   Name  \
742          743         1       1  Ryerson, Miss. Susan Parker "Suzette"   

        Sex   Age  SibSp  Parch    Ticket     Fare            Cabin Embarked  \
742  female  21.0      2      2  PC 17608  262.375  B57 B59 B63 B66        C   

     male_dummy  
742       False  

</pre>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PassengerId</th>
      <th>Survived</th>
      <th>Pclass</th>
      <th>Name</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Ticket</th>
      <th>Fare</th>
      <th>Cabin</th>
      <th>Embarked</th>
      <th>male_dummy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>742</th>
      <td>743</td>
      <td>1</td>
      <td>1</td>
      <td>Ryerson, Miss. Susan Parker "Suzette"</td>
      <td>female</td>
      <td>21.0</td>
      <td>2</td>
      <td>2</td>
      <td>PC 17608</td>
      <td>262.375</td>
      <td>B57 B59 B63 B66</td>
      <td>C</td>
      <td>False</td>
    </tr>
  </tbody>
</table>
</div>

<p>
Miss Ryerson has survived! Congratulations Suzette! And congratulations to our model which correctly predicted her survival. 
</p>


<p>
Back to varying our test size. If we choose a value of 0.4 for our test size, we get a slightly different outcome.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span style="color: #BA36A5;">P_titanic_train</span>, <span style="color: #BA36A5;">P_titanic_test</span>, <span style="color: #BA36A5;">q_titanic_train</span>, <span style="color: #BA36A5;">q_titanic_test</span> = \
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   train_test_split(P_titanic,q_titanic, test_size=0.4, random_state=42)
knn.fit(P_titanic_train, q_titanic_train)
<span style="color: #BA36A5;">prediction</span>= knn.predict(P_titanic_test)
knn.score(P_titanic_test, q_titanic_test)

</pre>
</div>

<pre class="example">
0.6756756756756757

</pre>

<div class="org-src-container">
<pre class="src src-ipython"><span style="color: #0000FF;">print</span>(<span style="color: #008000;">'Prediction{}'</span>.<span style="color: #006FE0;">format</span>(prediction))
</pre>
</div>

<pre class="example">
Prediction[1 1 1 0 0 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 1 1
 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 0 0 1 1 0 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1]


</pre>


<p>
A larger test set gives us a lower accuracy score, probably because we have a smaller training set. However, the accuracy score is not always reliable. See the <a href="#orga288730">Discussion</a> below for an explanation. It is not obvious which size gives the best result. We will use a test size of 0.2 for KNN and one of 0.25 for logistic regression. 
</p>

<p>
Now we introduce two methods to better evaluate the performances of our model. To prevent that our results are influenced by one particular way of splitting our data, we perform a technique called <i>cross-validation</i>. We ask ourselves the question: Do we see these results because we have accidentally chosen a very specific part of the data as our test set? Or is this a representative result of our entire dataset? This uncertainty can influence the reliability of our outcome. Using cross-validation we split our data into \(k\) folds and let our computer perform the algorithm \(k\) times on \(k\) different but equally large selections of our data of test and training sets. To illustrate, if we choose \(k\) is 5 we perform 5-fold cross-validation (see Figure <a href="#tab:5-foldcross">tab:5-foldcross</a>). Note, we are not gaining more accuracy with this technique for we are not using more data. The dataset stays the same. We get a better idea where our results come from. 
</p>

<p>
We use five different parts of our data as test set and the rest of the data as training set.
</p>



<div id="orgbb2da59" class="figure">
<p><img src="./CrossValidation.png" alt="CrossValidation.png" width="300px" />
</p>
<p><span class="figure-number">Figure 18: </span>5-fold cross-validation</p>
</div>


<p>
First, we split our data into five groups. We hold out the first fold as a test set, fit our model on the remaining four groups and we then predict on the first fold. In the next fold we use the second block as test set and fit on the remaining data and so on. Working with more folds is more computationally expensive and thus taking the computer longer to perform the cross-validation. 
</p>

<p>
To get an idea about how this cross-validation (cv) works, we perform cv with 5 folds. 
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span style="color: #0000FF;">from</span> sklearn.model_selection <span style="color: #0000FF;">import</span> cross_val_score
<span style="color: #BA36A5;">cv_scores</span> = cross_val_score(knn, P_titanic, q_titanic, cv=5, scoring=<span style="color: #008000;">'roc_auc'</span>)
<span style="color: #0000FF;">print</span>(cv_scores)
</pre>
</div>

<pre class="example">
[0.41666667 0.48833333 0.53833333 0.5        0.50694444]


</pre>

<p>
Here we see five values of \(R^{2}\) which is a statistical measure of how close the datapoints are to the fitted regression line. It is also known as the coefficient of determination.<sup><a id="fnr.16" class="footref" href="#fn.16">16</a></sup> 0% indicates that the model explains none of the variability of the response data around its mean, whereas 100% indicates that the model explains all the variability of the response data around its mean. For each of the different folds, we find that \(R^2\) is around 0.5. Hence, the particular fold chosen does not affect our results. Because if it did, we would have found for instance values of 0.1 next to values of 0.9. 
</p>

<p>
To get an idea what the influence is of different sizes of cross-validation on our score, we perform another cross-validation in the <a href="#orgb557d4b">Appendix</a>.  
</p>

<p>
The second method to evaluate our model's performance is the so-called confusion matrix. The confusion matrix is a table with four different combinations of predicted and actual values. The name stems from the fact that it makes it easy to see if the system is confusing two classes.<sup><a id="fnr.17" class="footref" href="#fn.17">17</a></sup> The four different combinations are: true positive (TP), true negative (TN), false positive (FP) and false negative (FN). The table has two dimensions: "actual" and "predicted". TP indicates that the algorithm predicted positive and that it was right. So this is a correct prediction that the passenger has survived. TN says that the algorithm predicted negative (so the passenger did not survive) and that the prediction was true. FP: the algorithm predicted positive but it is false. FN means that the algorithm predicted negative but was not right. For an example for the prediction of spam emails in a confusion matrix, see Figure <a href="#tab:matrix">tab:matrix</a> (borrowed from DataCamp).  
</p>

<div id="org29dfe40" class="figure">
<p><img src="./CONFUSIONMATRIX.png" alt="CONFUSIONMATRIX.png" width="300px" />
</p>
<p><span class="figure-number">Figure 19: </span>The confusion matrix</p>
</div>

<p>
Accuracy can be described as follows: 
</p>
\begin{equation}
accuracy = \frac{tp+tn}{tp+tn+fp+fn}
\end{equation}

<p>
We illustrate this method when we do the logistic regression. Although we can predict survival with KNN, it is not immediately clear what the effect is of <code>Age</code> and <code>Sex</code>. This is easier to see in logistic regression. 
</p>


<p>
\newpage
</p>
</div>
</div>
<div id="outline-container-org17606c2" class="outline-3">
<h3 id="org17606c2"><span class="section-number-3">4.2</span> The second algorithm: Logistic Regression</h3>
<div class="outline-text-3" id="text-4-2">
<p>
Another way to approach our problem is by using logistic regression (logreg for short). This is the algorithm that outputs probablities, which is exactly what we need in order to answer our main- and subquestions. We follow almost the same procedure as we did with KNN. We import the regressor from the library <code>sklearn.linear_model</code>. Thereafter, we split our dataset into training and test set, perform k-fold cross-validation, fit our regressor to the training set and predict on the test set. We choose 0.25 for our test size and 5 folds to split our dataset in training and test sets and print the five cross-validation scores. 
</p>


<div class="org-src-container">
<pre class="src src-ipython"><span style="color: #0000FF;">from</span> sklearn.linear_model <span style="color: #0000FF;">import</span> LogisticRegression
<span style="color: #BA36A5;">logreg</span> = LogisticRegression()
<span style="color: #0000FF;">from</span> sklearn.model_selection <span style="color: #0000FF;">import</span> train_test_split
<span style="color: #BA36A5;">P_titanic_train</span>, <span style="color: #BA36A5;">P_titanic_test</span>, <span style="color: #BA36A5;">q_titanic_train</span>, <span style="color: #BA36A5;">q_titanic_test</span> = \
train_test_split(P_titanic,q_titanic, test_size=0.25, random_state=42)
<span style="color: #0000FF;">from</span> sklearn.model_selection <span style="color: #0000FF;">import</span> cross_val_score
<span style="color: #BA36A5;">cv_scores</span> = cross_val_score(logreg, P_titanic, q_titanic, cv=5, scoring=<span style="color: #008000;">'roc_auc'</span>)
logreg.fit(P_titanic_train, q_titanic_train)
<span style="color: #BA36A5;">ylog_pred</span> = logreg.predict(P_titanic)
<span style="color: #0000FF;">print</span>(cv_scores)
</pre>
</div>

<pre class="example">
[0.86666667 0.80333333 0.74666667 0.73263889 0.92361111]


</pre>

<p>
Again the performance of the model does not depend a lot on the particular fold chosen. We see that these scores of \(R^{2}\) are higher than the ones we found using KNN. This algorithm performs better than KNN for the Titanic dataset. 
</p>

<p>
Here we come back to the confusion matrix to evaluate the quality of our model's prediction. We want our values on the diagonal to be as high as possible. A high number of values off the diagonal indicate problem areas. There are a lot of metrics that work with the classes in the confusion matrix in order to measure our model performance. A very popular metric for classifcation is the ROC (i.e. receiver operating characteristic) Curve and especially the area under this curve (AUC). This curve has to do with the threshold we set for our model. Using the logistic regression model, we have set our threshold at \(p=0.5\) (\(p<0.5\) indicates that the passenger has not survived and \(p>0.5\) that he has survived). So, what happens to our True Positive and False Positive rates when we vary this threshold? When \(p=0\), the model predicts 1 for all the data, which means the True Positive rate is equal to our False Positive rate which is equal to 1. When we set \(p=1\), the model predicts 0 for all the data. Both True and False Positive rates are 0. If we vary the threshold, we get a series of different True Positive and False Positive rates. The series of points we get when trying all possible thresholds are given in the plot titled 'Logistic Regression ROC Curve'. 
</p>

<p>
The larger the area under the ROC Curve, the better our model is. One way to understand this, is the following. We would have a great model if we had a model which produced an ROC Curve that had a single point in the upper left corner representing a True Positive rate of 1 and a False Positive Rate of 0. The ROC Curve is in the case of Figure <a href="#tab:auc">tab:auc</a>, the red line. The area under this curve (the light blue square) is at it's maximum. Therefore AUC is another popular metric for classification  models. 
</p>


<div id="org1b56164" class="figure">
<p><img src="./AUC2.png" alt="AUC2.png" width="300px" />
</p>
<p><span class="figure-number">Figure 20: </span>AUC</p>
</div>

<p>
To compute our AUC score, we program the following code: 
</p>
<div class="org-src-container">
<pre class="src src-ipython"><span style="color: #0000FF;">from</span> sklearn.metrics <span style="color: #0000FF;">import</span> roc_curve, auc
<span style="color: #BA36A5;">roc_auc</span> = auc(false_positive_rate, true_positive_rate)
roc_auc
</pre>
</div>

<pre class="example">
0.8154761904761904

</pre>

<p>
If the AUC is greater than 0.5, it means that our model is better than just random guessing. 
</p>
















<div class="org-src-container">
<pre class="src src-ipython">
<span style="color: #BA36A5;">y_pred_prob</span>=logreg.predict_proba(P_titanic_test)[:,1]
<span style="color: #BA36A5;">false_positive_rate</span>, <span style="color: #BA36A5;">true_positive_rate</span>, <span style="color: #BA36A5;">thresholds</span> = roc_curve(q_titanic_test, y_pred_prob)
plt.plot(false_positive_rate, true_positive_rate, label=<span style="color: #008000;">'LogisticRegression'</span>)
plt.xlabel(<span style="color: #008000;">'False Positive Rate'</span>)
plt.ylabel(<span style="color: #008000;">'True Positive Rate'</span>)
plt.title(<span style="color: #008000;">'Logistic Regression ROC Curve'</span>)
plt.show()

</pre>
</div>


<div class="figure">
<p><img src="obipy-resources/b207b8ecf66cdede2d5455fb7467ce47-17581ZrU.png" alt="b207b8ecf66cdede2d5455fb7467ce47-17581ZrU.png" />
</p>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span style="color: #BA36A5;">roc_auc</span> = auc(false_positive_rate, true_positive_rate)
roc_auc
</pre>
</div>

<pre class="example">
0.8154761904761904

</pre>



<div class="org-src-container">
<pre class="src src-ipython"><span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">print</span>(<span style="color: #008000;">'Prediction{}'</span>.<span style="color: #006FE0;">format</span>(ylog_pred))
</pre>
</div>

<pre class="example">
Prediction[1 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 0 1
 1 0 1 0 0 1 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1
 1 1 0 1 1 1 1 1 0 0 0 1 1 1 0 0 0 0 0 0 1 1 1 0 0 1 1 1 1 0 0 1 1 1 0 1 1
 0 1 1 1 1 0 1 1 0 1 0 1 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0
 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0]


</pre>

<p>
Here we see our predictions using logistic regression. This is the prediction for a fraction of 0.25 of our dataset. If we print our coefficients we get the following. 
</p>

<div class="org-src-container">
<pre class="src src-ipython">logreg.coef_
</pre>
</div>

<pre class="example">
array([[ 0.07374214,  0.00377371, -0.00684224, -2.0694906 ]])

</pre>


<p>
These coefficients correspond to the four columns of  <code>P_titanic</code>, which are <code>Pclass</code>, <code>Fare</code>, <code>Age</code> and <code>male_dummy</code> respectively (as seen in Table <a href="#tab:ptitanichead">tab:ptitanichead</a>). One can interpret the coefficients as follows: The higher your class, the higher your chance of survival (we call third class higher than first class). Same goes for <code>Fare</code>, because 0.00377371 is a positive number. We see that the coefficient corresponding to <code>Age</code> is negative, which indicates that the higher your age the lower your chance of surviving. In the case of <code>male_dummy</code>, the coefficient is negative as well which indicates that the chance of surviving decreases when <code>male_dummy</code> equals one. 
</p>

<p>
If we take a look at the coefficient corresponding to <code>Pclass</code> we see something counterintuitive. The positive coefficient 0.07374214 suggests that the higher the class, the higher the chance of survival. One might expect that the chance of survival is highest when travelling first class.
</p>

<p>
This paradox is resolved once we observe that the higher the fare, the higher the chance of survival. We have seen that plotting <code>Fare</code> against <code>Pclass</code> gives us a positive correlation. The coefficient of <code>Pclass</code> gives the effect of class on the chance of survival with a <b>given</b> fare, age and gender. A higher class with the same fare does not necessarily arise because there belongs a certain value of Fare to the first class: these two variables are positively correlated. When travelling first class instead of second class, two things change: the class and the price paid for a ticket (<code>Fare</code>). If we want to calculate the overall chance of surviving when travelling first class, we will have to take the effect of Fare into account as well.  
</p>

<p>
In order to solve this problem we will have a closer look at the dataset. We will group the mean of  <code>Fare</code>, <code>Age</code> and <code>male_dummy</code> by the column <code>Pclass</code> in Table <a href="#tab:tablegroupby">tab:tablegroupby</a>. 
</p>

<div class="org-src-container">
<pre class="src src-ipython">P_titanic.groupby(P_titanic.Pclass).mean()
</pre>
</div>

\begin{table}
\small
\begin{center}
\caption{\label{tab:tablegroupby}The mean of Fare, Age and male_dummy grouped by PClass}
\begin{tabular}{|l|c|c|c|}
\toprule
\hline
{} &       Fare &        Age &  male\_dummy \\
Pclass &            &            &             \\
\midrule
\hline
1      &  88.683228 &  37.591266 &    0.531646 \\
2      &  18.444447 &  25.266667 &    0.400000 \\
3      &  11.027500 &  21.000000 &    0.500000 \\
\bottomrule
\hline
\end{tabular}
\end{center}
\end{table}



<p>
Here we calculate the effect on the chances of survival of someone travelling first, second or third class that paid the average fare 
</p>
<div class="org-src-container">
<pre class="src src-ipython"><span style="color: #0000FF;">print</span>(0.07374214*1+0.00377371*88.683228)
<span style="color: #0000FF;">print</span>(0.07374214*2+0.00377371*18.444447)
<span style="color: #0000FF;">print</span>(0.07374214*3+0.00377371*11.027500)
</pre>
</div>

<pre class="example">
0.40840692433588
0.21708827408837
0.26284100702499996


</pre>

<p>
The chance of surviving the disaster when travelling first class and having paid the average fare is 0.41. The chance of surviving the disaster when travelling second class and having paid the average fare is 0.22. In the case of travelling third class the chance of surviving is 0.26. 
</p>

<p>
The fact that the variables influence each other will be discussed further in the <a href="#orga288730">Discussion</a>. 
</p>

<p>
For the actual chances of survival we have to multiply our coefficients with the corresponding <code>Age</code>, <code>Class</code>, <code>Fare</code> and <code>Sex</code>. After this we calculate the chance by putting these numbers in the equation of logistic regression. We have set the threshold at \(p = 0.5\), which means that if \(p < 0.5\),  we will see a zero in our prediction which indicates that the passenger has not survived the disaster. To calculate the chance of survival for, for instance, the woman of 40 years old travelling first class we get: 
</p>


<div class="org-src-container">
<pre class="src src-ipython"><span style="color: #BA36A5;">t</span>= 0.07374214*1+0.00377371*88.683228 - 0.0068422 * 40 -2.0694906*0

</pre>
</div>

\begin{equation}
t= 0.07374214*1+0.00377371*88.683228 - 0.0068422 * 40 -2.0694906*0
\end{equation}

<p>
We fill in this value for \(t\) in our sigmoid function. 
</p>


\begin{equation}
\label{sigmoid}
\sigma_t = \frac{e^{0.1347189243}}{1+e^{0.1347189243}}
\end{equation}

<p>
This gives us a chance of 0.53362885 \(\approx\) 0.53. This is higher than 0.5, which means that the woman of 40 years old travelling first class was likely to have survived. However, we do not need to calculate all the chances of survival for each particular passenger to find an answer to our main question and sub-questions. The logreg coefficients tell us enough about the influence of the chosen variables on the chance of surviving the disaster of the Titanic. See the following chapter <a href="#org12876ed">Conclusion</a> for the answer to our main question and sub-questions.  
</p>





















<p>
\newpage
</p>
</div>
</div>
</div>
<div id="outline-container-orgc40f262" class="outline-2">
<h2 id="orgc40f262"><span class="section-number-2">5</span> Conclusion</h2>
<div class="outline-text-2" id="text-5">
<p>
<a id="org12876ed"></a>
The goal of this paper is to answer the following questions and compare the answers to the hypotheses. 
</p>

<p>
<b>Main question</b>
</p>

<p>
<i>Is it possible to make an accurate prediction whether the passengers on board of the Titanic survived the disaster or not using the information about gender, class and age given in the dataset?</i>
</p>

<p>
Yes, this is indeed possible with the help of machine learning algorithms such as KNearestNeighbours and logistic regression. For our data logreg performs better than KNN. . 
</p>

<p>
<b>Sub-questions</b> 
</p>

<ul class="org-ul">
<li><i>What is the influence of gender on the chance of surviving after the Titanic had sunk?</i></li>
</ul>
<p>
The coefficient of the logistic regression suggests that women had a higher chance of surviving than men. 
</p>


<ul class="org-ul">
<li><i>What is the influence of class on the chance of surviving after the Titanic had sunk?</i></li>
</ul>
<p>
Looking at the positive correlation between <code>Fare</code> and <code>Pclass</code> and the fact that a higher Fare increased the chance of surviving, the lower your class (1 is lower than 3),  the higher your fare and therefore the higher your chance of surviving. 
</p>

<ul class="org-ul">
<li><i>What is the influence of age on the chance of surviving after the Titanic had sunk?</i></li>
</ul>
<p>
The higher your age, the lower your chance of survival. 
</p>

<p>
We formulated the following hypotheses: 
</p>

<ul class="org-ul">
<li><b>Main question</b> : Yes this is possible, with the help of machine learning using the algorithms KNearestNeighbours and logistic regression.</li>
<li><b>Sub-questions</b> :

<ul class="org-ul">
<li>The survival rate of women is higher than the survival rate of men.</li>
<li>The survival rate of passengers who were travelling in a higher class is higher than those travelling in a lower class.</li>
<li>The survival rate of children and elderly is higher than the survival rate of the adults.</li>
</ul></li>
</ul>

<p>
Our conclusions correspond to all of our hypotheses. 
</p>

<p>
\newpage
</p>
</div>
</div>
<div id="outline-container-org19ef723" class="outline-2">
<h2 id="org19ef723"><span class="section-number-2">6</span> Discussion</h2>
<div class="outline-text-2" id="text-6">
<p>
<a id="orga288730"></a>
</p>

<p>
A number of factors can have an influence on our model's prediction. To begin with, the more variables we use, the better  our model predicts on the training set. This is evident, because using more information given in the dataset will give the computer more details to create a fitting algorithm. The downside is that more variables can lead to overfitting. We see patterns in our training set that are not present in our whole dataset.
</p>

<p>
The emphasis of this paper was not necessarily on the precision of our algorithm but on learning the basics and getting a taste of machine learning. Because of this and because of the deadlines, the choice was made to go with the four variables <code>Fare</code>, <code>Age</code>, <code>Gender</code> and <code>Class</code>. To make the algorithm more reliable we can use more variables next time.  
</p>

<p>
When we want to grade our model performance, we use classification metrics such as accuracy. In the case of accuracy there is a catch however. For instance, if we take a look at spam classification. 99% of the emails we receive is real and 1% is spam. We instantiate a classifier which classifies all emails as real. Computing the accuracy will give us a score of 99%, which is pretty high, but our classifier is horrible at predicting spam. This is less of an issue in our dataset where the sinking of the RMS Titanic killed 1502 out of the 2224 people on board, which is 68%. We can use more nuanced metrics the next time with help of the confusion matrix. The negative predictive value (NPV), for example, is another metric from the confusion matrix (see Figure <a href="#tab:matrix">tab:matrix</a>)
</p>
\begin{equation}
TNR = \frac{tn}{tn+fn}
\end{equation} 

<p>
In the case of our horrible spam predictor, the NPV rate is zero and shows that our model is not able to classify the 1% spam. Including this metric will give us a more reliable interpretation of our model. 
</p>

<p>
Furthermore, the variables influence each other (indirectly) as well. A higher fare is more likely to pair with a passenger travelling first class. Not all combinations of  fare and class are in the data. We have to take this into account when interpreting the coefficients. Although, this does not affect our algorithm. An example of this was the coefficient corresponding to <code>Pclass</code>. It seemed counterintuitive at first that a higher class (in this case third class is higher than first class) gives a higher chance of survival, because we expected that first class would increase the chance of survival. After we found that when travelling first class instead of second class, two things change (i.e. the class and the price paid for a ticket (<code>Fare</code>)), we find that a higher <code>Fare</code> pairs with a higher chance of survival and consequently with a lower class. So indirectly lower class and a higher price paid for a ticket increases the chance of survival which makes more sense. Still, it is important to note that it is not easy to draw conclusions from the coefficients. 
</p>

<p>
Finally, we have used a statistical model to explain chances of survival on board of the Titanic. We need to keep in mind that during a disaster chance, impulsivity and unexpected actions play a role. Our model will never match the exact situation during the demise of the Titanic. Furthermore, the goal is also not to make predictions about disasters and therefore making predictions about future catastrophes. The goal is that these results teach us something about the circumstances during that period. It teaches us something about the society in 1912. We can, for example, assume that the women and children first policy did work, because we have seen that the higher your age, the lower your chance of survival. Similarly, women had a higher chance of survival. 
</p>


<p>
\newpage
</p>
</div>
</div>
<div id="outline-container-org6973c2f" class="outline-2">
<h2 id="org6973c2f"><span class="section-number-2">7</span> Postface</h2>
<div class="outline-text-2" id="text-7">
<p>
Writing this paper has taught me a lot. Before I started, I did not know anything about programming or machine learning. With the help of DataCamp and my father I have experienced what it is to write code. The job prospects of programmers who know how to deal with a lot of data and machine learning algorithm are pretty good. Python is one of the easiest and most accessible languages to learn how to program. So knowing your way around machine learning is a good idea. In addition, a lot of big steps are made in the field of artificial intelligence.  Big tech giants such as Netflix, Apple and Google are looking for faster and more efficient ways of making our life on the internet easier.  Furthermore, there are not a lot of girls who take interest in subjects as these so I wanted to show that it is not impossible for a girl to enjoy machine learning and programming. During the presentation of my paper I want to show that programming is fun for boys and girls of our age. Lastly, writing this paper in English has given me a head start in my career, because I am sure that I will write more papers in English in the future. My plan is to get my PhD after my study at TU Eindhoven. 
</p>

<p>
All in all, writing this paper was a lot of fun. I have learned a lot and I think it has been a great preparation for my education at TU Eindhoven. 
</p>
</div>
</div>


<div id="outline-container-org9d8e3ce" class="outline-2">
<h2 id="org9d8e3ce"><span class="section-number-2">8</span> References</h2>
<div class="outline-text-2" id="text-8">
<p>
\printbibliography
</p>
<ul class="org-ul">
<li><a href="https://www.kaggle.com/c/titanic#evaluation">https://www.kaggle.com/c/titanic#evaluation</a></li>
<li><a href="https://www.datacamp.com/home">https://www.datacamp.com/home</a></li>
</ul>

<p>
\newpage
</p>

<p>
\newpage
</p>
</div>
</div>
<div id="outline-container-org3ec1bbb" class="outline-2">
<h2 id="org3ec1bbb"><span class="section-number-2">9</span> Appendix</h2>
<div class="outline-text-2" id="text-9">
<p>
<a id="orgb557d4b"></a>
</p>
</div>

<div id="outline-container-org128917f" class="outline-3">
<h3 id="org128917f"><span class="section-number-3">9.1</span> Some more plots</h3>
<div class="outline-text-3" id="text-9-1">
<p>
A plot of <code>Fare</code> against <code>Survived</code>: 
</p>
<div class="org-src-container">
<pre class="src src-ipython"><span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   sns.<span style="color: #006FE0;">set</span>(style=<span style="color: #008000;">"darkgrid"</span>)
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">ax</span> = sns.countplot(x=<span style="color: #008000;">"Fare"</span>,hue=<span style="color: #008000;">"Survived"</span>, data=data)

<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   ax.set_xticks([])
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   
</pre>
</div>

<pre class="example">
[]

</pre>




<div class="figure">
<p><img src="obipy-resources/b207b8ecf66cdede2d5455fb7467ce47-17581m1a.png" alt="b207b8ecf66cdede2d5455fb7467ce47-17581m1a.png" />
</p>
</div>

<p>
We can see that the quantity of green points increases if <code>Fare</code> increases. The ratio of green vs. blue increases with Fare. Which means that you were more likely to survive if you paid a higher ticket price. 
</p>

<p>
\newpage
</p>
</div>
</div>
<div id="outline-container-orgbb5676d" class="outline-3">
<h3 id="orgbb5676d"><span class="section-number-3">9.2</span> Some more cross-validation</h3>
<div class="outline-text-3" id="text-9-2">
<p>
We will now perform 10-fold cv. This way we are able to compare the results and have a look at what the influence of more folds is on our values of \(R^{2}\).
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span style="color: #BA36A5;">cv_scores</span> = cross_val_score(knn, P_titanic, q_titanic, cv=10, scoring=<span style="color: #008000;">'roc_auc'</span>)
<span style="color: #0000FF;">print</span>(cv_scores)
</pre>
</div>

<pre class="example">
[0.37179487 0.57692308 0.55128205 0.63888889 0.36111111 0.64583333
 0.54166667 0.44444444 0.52083333 0.54861111]


</pre>



<p>
Our array consists of ten columns, which makes sense because we split our data in ten different ways. As in the main text, we find dat \(R^2\) is around 0.5, hence this result is robust. 
</p>
</div>
</div>
</div>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" class="footnum" href="#fnr.1">1</a></sup> <div class="footpara"><a href="https://en.wikipedia.org/wiki/RMS_Titanic#Maiden_voyage">https://en.wikipedia.org/wiki/RMS_Titanic#Maiden_voyage</a> (consulted on the 5th of August, 2018).</div></div>

<div class="footdef"><sup><a id="fn.2" class="footnum" href="#fnr.2">2</a></sup> <div class="footpara"><a href="https://www.encyclopedia-titanica.org/titanic/">https://www.encyclopedia-titanica.org/titanic/</a> (consulted on the 5th of August, 2018).</div></div>

<div class="footdef"><sup><a id="fn.3" class="footnum" href="#fnr.3">3</a></sup> <div class="footpara"><a href="http://www.bbc.co.uk/history/titanic">http://www.bbc.co.uk/history/titanic</a> (consulted on the 5th of August, 2018).</div></div>

<div class="footdef"><sup><a id="fn.4" class="footnum" href="#fnr.4">4</a></sup> <div class="footpara"><a href="https://en.wikipedia.org/wiki/RMS_Titanic#Maiden_voyage">https://en.wikipedia.org/wiki/RMS_Titanic#Maiden_voyage</a> (consulted on the 5th of August, 2018).</div></div>

<div class="footdef"><sup><a id="fn.5" class="footnum" href="#fnr.5">5</a></sup> <div class="footpara"><a href="https://www.kaggle.com/c/titanic">https://www.kaggle.com/c/titanic</a> (consulted on the 5th of August, 2018).</div></div>

<div class="footdef"><sup><a id="fn.6" class="footnum" href="#fnr.6">6</a></sup> <div class="footpara"><a href="http://www.bbc.co.uk/history/titanic">http://www.bbc.co.uk/history/titanic</a> (consulted on the 5th of August, 2018).</div></div>

<div class="footdef"><sup><a id="fn.7" class="footnum" href="#fnr.7">7</a></sup> <div class="footpara">Quote created by Stanford University on the course of Machine Learning, taught by: Andrew Ng, Co-founder, Coursera; Adjunct Professor, Stanford University; formerly head of Baidu AI Group/Google Brain. <a href="https://www.coursera.org/learn/machine-learning">https://www.coursera.org/learn/machine-learning</a> (consulted on the 6th of August, 2018).</div></div>

<div class="footdef"><sup><a id="fn.8" class="footnum" href="#fnr.8">8</a></sup> <div class="footpara"><a href="https://www.redpixie.com/blog/examples-of-machine-learning">https://www.redpixie.com/blog/examples-of-machine-learning</a>(consulted on the 6th of August, 2018).</div></div>

<div class="footdef"><sup><a id="fn.9" class="footnum" href="#fnr.9">9</a></sup> <div class="footpara"><a href="https://deepmind.com/blog/alphago-zero-learning-scratch/">https://deepmind.com/blog/alphago-zero-learning-scratch/</a>(consulted on the 6th of August, 2018).</div></div>

<div class="footdef"><sup><a id="fn.10" class="footnum" href="#fnr.10">10</a></sup> <div class="footpara">New Scientist Weekly, 21 July 2018, I teach machines to hunt down cancer, Interview by Chelsea Whyte</div></div>

<div class="footdef"><sup><a id="fn.11" class="footnum" href="#fnr.11">11</a></sup> <div class="footpara">Machine Learning, An Algorithmic Perspective second edition by Stephen Marsland, 2015 by Taylor &amp; Francis Group.</div></div>

<div class="footdef"><sup><a id="fn.12" class="footnum" href="#fnr.12">12</a></sup> <div class="footpara"><a href="https://machinelearningmastery.com/supervised-and-unsupervised-machine-learning-algorithms/">https://machinelearningmastery.com/supervised-and-unsupervised-machine-learning-algorithms/</a>(consulted on the 26th of August, 2018).</div></div>

<div class="footdef"><sup><a id="fn.13" class="footnum" href="#fnr.13">13</a></sup> <div class="footpara">DataCamp courses on Supervised Learning with scikitlearn: <a href="https://www.datacamp.com/courses/q:supervised">https://www.datacamp.com/courses/q:supervised</a> (consulted on the 13th of February, 2018). \label{fn:datacamp}</div></div>

<div class="footdef"><sup><a id="fn.14" class="footnum" href="#fnr.14">14</a></sup> <div class="footpara"><a href="https://www.statisticssolutions.com/what-is-logistic-regression/">https://www.statisticssolutions.com/what-is-logistic-regression/</a>(consulted on the 5th of September, 2018).</div></div>

<div class="footdef"><sup><a id="fn.15" class="footnum" href="#fnr.15">15</a></sup> <div class="footpara"><a href="https://www.kaggle.com/c/titanic">https://www.kaggle.com/c/titanic</a> (consulted on the 18th of January 2018)</div></div>

<div class="footdef"><sup><a id="fn.16" class="footnum" href="#fnr.16">16</a></sup> <div class="footpara"><a href="https://www.datasciencecentral.com/profiles/blogs/regression-analysis-how-do-i-interpret-r-squared-and-assess-the">https://www.datasciencecentral.com/profiles/blogs/regression-analysis-how-do-i-interpret-r-squared-and-assess-the</a> (consulted on the 10th of December, 2018)</div></div>

<div class="footdef"><sup><a id="fn.17" class="footnum" href="#fnr.17">17</a></sup> <div class="footpara"><a href="https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62">https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62</a> (consulted on the 2nd of December, 2018)</div></div>


</div>
</div></div>
<div id="postamble" class="status">
<p class="author">Author: Myrthe Boone</p>
<p class="date">Created: 2019-02-01 Fri 12:22</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
